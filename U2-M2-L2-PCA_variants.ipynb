{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All).\n",
    "\n",
    "**IMPORTANT: DO NOT COPY OR SPLIT CELLS.** If you do, you'll mess the autograder. If need more cells to work or test things out, create a new cell. You may add as many new cells as you need.\n",
    "\n",
    "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\", as well as your name and group below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COURSE = \"Unsupervised Learning 2021\"\n",
    "GROUP = \"D8A\"\n",
    "NAME = \"\" # Match your GitHub Classroom ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Robust Principal Component Analysis\n",
    "\n",
    "## Implementation (6 pts)\n",
    "\n",
    "In this exercise, you will implement RPCA using the ADMM method reviewd in class. Then you will test it with synthetic data sets, and with handwritten digits.\n",
    "\n",
    "Choose the step size for the ADMM algorithm as $\\rho = \\frac{nd}{4|M|_1}$.\n",
    "\n",
    "Terminate the algorithm when $|M - L - S|_F \\leq \\delta |M|_F$, with $\\delta=10^{-7}$.\n",
    "\n",
    "This implementation is slow for large matrice, we could use a partial SVD to speed things up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f88d0c3f7d817d8293608e03dd2a8d33",
     "grade": false,
     "grade_id": "ex-1",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Implement RPCA using the ADMM optimizing algorithm.\n",
    "\n",
    "def soft_t(X, t):\n",
    "    # Implement soft-thresholding\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "def SVT(L, t, k):\n",
    "    # Implement Singular value thresholding\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "\n",
    "def rpca(M, maxiter=10000):\n",
    "    \n",
    "    # You need to return the following variables correctly\n",
    "    L = np.zeros(M.shape)\n",
    "    S = np.zeros(M.shape)\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return L, S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a synthetic low rank matrix of size nxn with n = 100\n",
    "n = 100\n",
    "r = int(0.05*n) # Rank\n",
    "k = int(0.05*n**2) # Number of non-zero entries of S\n",
    "X = np.random.normal(loc=0, scale=1/500, size=(n,r))\n",
    "Y = np.random.normal(loc=0, scale=1/500, size=(n,r))\n",
    "E = (-1)**np.random.randint(0, 2, size=(n, n))\n",
    "S = np.zeros((n,n))\n",
    "mask = np.array([True]*k + [False]*(n*n-k))\n",
    "np.random.shuffle(mask)\n",
    "mask = mask.reshape((n,n))\n",
    "S[mask] = E[mask]\n",
    "\n",
    "M = X @ Y.T + S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0b276e36afbbef4ee10ba90e6c84f684",
     "grade": true,
     "grade_id": "ex-1-answer",
     "locked": true,
     "points": 6,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Check succesful recovery\n",
    "Lr, Sr = rpca(M)\n",
    "assert np.allclose(Lr, X@Y.T, atol=1e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1 (2 pts)\n",
    "\n",
    "Verify that the solution that minimizes\n",
    "\n",
    "\\begin{align}\n",
    "\\lambda|S|_1 + \\frac{\\rho}{2}\\left|S - M + L_{k+1} - Y_k/\\rho) \\right|_F^2\n",
    "\\end{align}\n",
    "\n",
    "with respect to $S$ is given by\n",
    "\n",
    "$$\n",
    "S_{k+1} = S_{\\frac{\\lambda}{\\rho}}\\left( M - L_{k+1} + \\frac{Y_k}{\\rho}  \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "62a566e7580c3a556c192a7243f5471c",
     "grade": true,
     "grade_id": "ans-q1",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1 (2 pts)\n",
    "\n",
    "The right eigenvectors of the decomposition $\\phi(X) = UDV^T$, i.e., the eigenvectors (loadings) in feature space, can be expanded in terms of the basis of observations,\n",
    "\n",
    "$$\n",
    "v_m = \\sum_{j=1}^n \\alpha_{jm}\\phi(x_j)\n",
    "$$\n",
    "\n",
    "Show that the principal components for KPCA are given by\n",
    "\n",
    "$$\n",
    "z_{im} = v_m^T \\phi(x_i) = \\sum_{j=1}^n \\alpha_{jm}\\phi(x_j)^T \\phi(x_i)\n",
    "= \\sum_{j=1}^n \\alpha_{jm}K(x_i, x_j)\n",
    "$$\n",
    "\n",
    "with $\\alpha_{jm} = u_{jm}/d_m$, assume a centered K matrix. This is, the vector of coefficients, $\\alpha_m$ is equal to the $m$ eigenvector divided by the square of the eigenvalue, $\\alpha_m = u_m/d_m$.\n",
    "\n",
    "Also show that if the K matrix is not centered, we can still project any existing observation by centering K before, and any new observation $x_0$, by applying\n",
    "\n",
    "$$\n",
    "\\vec{z}_{m} = A (I - M)\\left(\\vec{k}_0 - \\frac{1}{N} K \\vec{1}\\right)\n",
    "$$\n",
    "\n",
    "where $A$ is the matrix of column vectors $\\alpha_m$ given by $A=U D^{-1}$, and $k_0$ is a vector of kernel products $\\phi(x_0)^T\\phi(x_i)$ with all observations $x_i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "319229657818ffc7eb813370a0b4c674",
     "grade": true,
     "grade_id": "ans-q2",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
